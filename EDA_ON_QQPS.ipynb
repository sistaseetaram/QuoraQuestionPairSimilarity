{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "### Description\n",
    "*** Ref: https://www.kaggle.com/c/quora-question-pairs/***\n",
    "\n",
    "* Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.<br/> <br/>\n",
    "\n",
    "* Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "\n",
    "### Problem Statement\n",
    "* Identify which questions asked on Quora are duplicates of questions that have already been asked. This could be useful to instantly provide answers to questions that have already been answered. <br/>\n",
    "** We are tasked with predicting whether a pair of questions are duplicates or not.**\n",
    "\n",
    "### Identifying Business Constraints Problem Formulation\n",
    "1. The cost of a mis-classification can be very high. i.e False Positives should be highly penalised, we do not want to link dissimilar questions together as the trust of user will reduce because of this.\n",
    "2. We would want a probability of a pair of questions to be duplicates so that you can choose any threshold of choice.\n",
    "3. No strict latency concerns.\n",
    "4. Interpretability is partially important.\n",
    "5. It is a binary classification problem, for a given pair of questions we need to predict if they are duplicate or not.\n",
    "\n",
    "### Data overview\n",
    "- Data will be in a file Train.csv <br>\n",
    "- Train.csv contains 5 columns : qid1, qid2, question1, question2, is_duplicate <br>\n",
    "- Size of Train.csv - 60MB <br>\n",
    "- Number of rows in Train.csv = 404,290\n",
    "\n",
    "### Performance Metric (decided by kaggle)\n",
    "Source: https://www.kaggle.com/c/quora-question-pairs#evaluation\n",
    "\n",
    "Metric(s): \n",
    "1. log-loss : https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "2. Binary Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import os\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import pickle as pk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display,Markdown\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "os.chdir('E:/Projects/QuoraQuestionPairSimilarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "print(\"Number of data points:\",df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34zXGW8gs5Wj",
    "outputId": "ab7d570a-9eeb-477a-b7cb-663ff6fd04fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mx4DFwMns5Wp",
    "outputId": "1141e0bb-2750-489e-8b8c-2ba680f7416c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHHTGTzws5Ww"
   },
   "source": [
    "### Observations:\n",
    "We are given a minimal number of data fields here, consisting of:\n",
    "\n",
    "- id:  Looks like a simple rowID\n",
    "- qid{1, 2}:  The unique ID of each question in the pair\n",
    "- question{1, 2}:  The actual textual contents of the questions.\n",
    "- is_duplicate:  The label that we are trying to predict - whether the two questions are duplicates of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZulqVzTDs5Wx"
   },
   "source": [
    "### Distribution of data points among output classes\n",
    "- Number of duplicate(smilar) and non-duplicate(non similar) questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHp64yNjs5Wx",
    "outputId": "361ddf04-d545-45f9-dbe2-8bebd695e8da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"is_duplicate\")['id'].count().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiPia6Pjs5W_",
    "outputId": "3cde4cec-4314-4c14-e807-b35e969bf9e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('''### Observations: \n",
    "* Total number of question pairs for training:\\n   {}\n",
    "* Question pairs are not Similar (is_duplicate = 0):\\n   {}%\n",
    "* Question pairs are Similar (is_duplicate = 1):\\n   {}%'''.format(len(df),100 - round(df['is_duplicate'].mean()*100, 2),round(df['is_duplicate'].mean()*100, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGX03QVRs5XF"
   },
   "source": [
    "### Number of unique questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOKa6aU2s5XG",
    "outputId": "8f644b1d-27c0-4d63-84e2-bb2a42419be2"
   },
   "outputs": [],
   "source": [
    "qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
    "unique_qs = len(np.unique(qids))\n",
    "qs_morethan_onetime = np.sum(qids.value_counts() > 1)\n",
    "\n",
    "q_vals=qids.value_counts()\n",
    "\n",
    "q_vals=q_vals.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCiDBHm5s5XT",
    "outputId": "d8011926-4086-4c9a-9fcf-59663a584ec4"
   },
   "outputs": [],
   "source": [
    "#checking whether there are any repeated pair of questions\n",
    "pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plcvbd4Cs5XM",
    "outputId": "8e137cc1-e0c4-44f4-9cc2-703302206d4f"
   },
   "outputs": [],
   "source": [
    "x = [\"unique_questions\" , \"Repeated Questions\"]\n",
    "y =  [unique_qs , qs_morethan_onetime]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title (\"Plot representing unique and repeated questions  \")\n",
    "sns.barplot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaHTnnt8s5XX"
   },
   "source": [
    "### Number of occurrences of each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPZwk-C8s5Xa",
    "outputId": "0d6d5978-2306-4ed3-cf27-f2a0b974e47d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.hist(qids.value_counts(), bins=160)\n",
    "\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.title('Log-Histogram of question appearance counts')\n",
    "\n",
    "plt.xlabel('Number of occurences of question')\n",
    "\n",
    "plt.ylabel('Number of questions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_WdYxlYs5Xj"
   },
   "source": [
    "### Checking for NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0x1gR2fs5Xk",
    "outputId": "721aef48-e628-40c6-d567-25466f4283e1"
   },
   "outputs": [],
   "source": [
    "#Checking whether there are any rows with null values\n",
    "nan_rows = df[df.isnull().any(1)]\n",
    "print (nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLBRyACgs5Xp",
    "outputId": "076046a9-1510-41ef-cf98-15b38661dca4"
   },
   "outputs": [],
   "source": [
    "# Filling the null values with ' '\n",
    "df = df.fillna('')\n",
    "nan_rows = df[df.isnull().any(1)]\n",
    "print (nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('''### Observations:\n",
    "* Total number of  Unique Questions are: {0}\n",
    "* Number of unique questions that appear more than one time: {1} ({2}%)\n",
    "* Max number of times a single question is repeated: {3}\n",
    "* Number of duplicate questions {4}\n",
    "* There are two rows with null values in question2 and one null value in question1 '''.format(unique_qs,qs_morethan_onetime,round(qs_morethan_onetime/unique_qs*100,3),max(qids.value_counts()),(pair_duplicates).shape[0] - df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Observation On qid's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref: https://www.kaggle.com/ashhafez/temporal-pattern-in-train-response-rates\n",
    "\n",
    "**If we make the assumption that there's an underlying temporal pattern to the data, and use the qid values as a proxy for it (higher qid value implies more recent question), then re-sorting the train set by increasing qid and plotting the sliding window of mean response rate should show us some pattern.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"qmax\"]      = df.apply( lambda row: max(row[\"qid1\"], row[\"qid2\"]), axis=1 )\n",
    "df              = df.sort_values(by=[\"qmax\"], ascending=True)\n",
    "df[\"dupe_rate\"] = df.is_duplicate.rolling(window=500, min_periods=500).mean()\n",
    "df[\"timeline\"]  = np.arange(df.shape[0]) / float(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"timeline\", y=\"dupe_rate\", kind=\"line\")\n",
    "plt.ylabel(\"dupe_rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- The above pattern, and the ~16.5% LB response rate reported by others, imply that the Public LB (and possibly Private LB) are potentially sourced from more recent data than the training set.\n",
    "-  the decreasing average duplicate ratio(rolling mean) with increasing QID — Mostprobably an indication of Quora’s improving algorithm with time, thus reducing the number of duplicate questions with increasing ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: I came up with following features after a lot of reading of kaggle discussions and after lots of hit and trail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9Qcl5xfs5Xs"
   },
   "source": [
    "### Basic Feature Extraction (before cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRzvPYzGs5Xu"
   },
   "source": [
    "I used following features:\n",
    " - ____freq_qid1____ = Frequency of qid1's\n",
    " - ____freq_qid2____ = Frequency of qid2's \n",
    " - ____q1len____ = Length of q1\n",
    " - ____q2len____ = Length of q2\n",
    " - ____q1_n_words____ = Number of words in Question 1\n",
    " - ____q2_n_words____ = Number of words in Question 2\n",
    " - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n",
    " - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n",
    " - ____word_share____ = (word_common)/(word_Total)\n",
    " - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n",
    " - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq4DZ-rYs5Xv",
    "outputId": "d34e66da-d84b-49ea-8852-4beb9da688ba",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ref: https://towardsdatascience.com/apply-function-to-pandas-dataframe-rows-76df74165ee4\n",
    "# Ref: https://stackoverflow.com/questions/33518124/how-to-apply-a-function-on-every-row-on-a-dataframe\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.pkl'):\n",
    "    df = pd.read_pickle(\"df_fe_without_preprocessing_train.pkl\")\n",
    "    print(\"Loaded file!!\")\n",
    "elif isinstance(df,pd.core.frame.DataFrame):\n",
    "    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
    "    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
    "    df['q1len'] = df['question1'].str.len() \n",
    "    df['q2len'] = df['question2'].str.len()\n",
    "    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "\n",
    "    def normalized_word_Common(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)\n",
    "    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
    "\n",
    "    def normalized_word_Total(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
    "\n",
    "    def normalized_word_share(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "    df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
    "\n",
    "    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
    "    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
    "\n",
    "    df.to_pickle(\"df_fe_without_preprocessing_train.pkl\")\n",
    "else:\n",
    "    print(\"Unexpected input encountered\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic_features = df[[\"freq_qid1\",\"freq_qid2\",\"q1len\",\"q2len\",\"q1_n_words\",\"q2_n_words\",\"word_Common\",\"word_Total\",\"word_share\",\"freq_q1+q2\",\"freq_q1-q2\"]]\n",
    "df_basic_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zLujovVs5X3"
   },
   "source": [
    "### Analysis of some of the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSS0X82Ds5X5",
    "outputId": "5dacd7b2-d955-4435-9639-f1c6acd9b580",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('''### Observations:\n",
    "1. Here some questions have only one single word and some also have one letter.\n",
    "2. Minimum  and Maximum no.of words of the questions in question1 respectively are {} and {}\n",
    "3. Minimum and Maximum no.of words of the questions in question2 respectively are {} and {}\n",
    "4. Number of Questions with minimum length [question1] : {}\n",
    "5. Number of Questions with minimum length [question2] : {}\n",
    "6. The Above Values show high variance in the dataset.\n",
    "7. I may be adding multicollinearity to the data which I've dealt before finalizing my train data.'''.format(min(df['q1_n_words']),max(df['q1_n_words']),min(df['q2_n_words']),max(df['q2_n_words']),df[df['q1_n_words']== 1].shape[0],df[df['q2_n_words']== 1].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4rwGLFDs5YD",
    "outputId": "0103aaa0-3f5a-4eb4-cd22-164a57d7aef0"
   },
   "outputs": [],
   "source": [
    "f= plt.figure(figsize=(12, 8))\n",
    "f.suptitle('Feature: word_share')\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\n",
    "sns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mCFvztcs5YM",
    "outputId": "008ac763-a832-4c11-88fa-5da52cdb9305",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "f.suptitle('Feature: word_Common')\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:] , label = \"1\", color = 'red')\n",
    "sns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:] , label = \"0\" , color = 'blue' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ej1ouEVs5YR"
   },
   "source": [
    "### Observations\n",
    "- The distributions for normalized word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity.\n",
    "- The distributions for normalized word_share w.r.t hue=\n",
    "- The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar)\n",
    "- The distributions of the word_Common feature in similar and non-similar questions are highly overlapping.Thus, this feature may give less feature importance.\n",
    "- drop word_common as it has high overlap. Let's take some other transformation of in feature eng stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Text\n",
    "** This Involves: **\n",
    "- Removing html tags \n",
    "- Removing Punctuations\n",
    "- Performing stemming\n",
    "- Removing Stopwords\n",
    "- Expanding contractions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reason Why I Used WordNet ref: https://www.kaggle.com/c/quora-question-pairs/discussion/30340\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib as jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the results in 4 decemal points\n",
    "# Ref: https://stackoverflow.com/questions/18603270/progress-indicator-during-pandas-operations\n",
    "class Preprocessor:\n",
    "    def __init__(self,lemma):\n",
    "        self.lemma=lemma\n",
    "\n",
    "    def preprocess(self,x):\n",
    "        '''This fuction is used to clan data and replace text with some common conventions used in emperical use of language.'''\n",
    "        x = str(x).lower()\n",
    "        x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                               .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                               .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                               .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                               .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                               .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                               .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "        x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "        x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "\n",
    "\n",
    "        if self.lemma:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "        else:\n",
    "            porter = PorterStemmer() \n",
    "        pattern = re.compile('\\W')\n",
    "\n",
    "        if isinstance(x,str):\n",
    "            x = re.sub(pattern, ' ', x)    \n",
    "        if isinstance(x,str):\n",
    "            x = ' '.join(list(map(lambda y: lemmatizer.lemmatize(y),x.split(' ')))) if self.lemma else ' '.join(list(map(lambda y: porter.stem(y),x.split(' '))))\n",
    "            example1 = BeautifulSoup(x)\n",
    "            x = example1.get_text()\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def call_preprocess(self,data):\n",
    "        tqdm.pandas()\n",
    "        temp = 'lemma' if self.lemma else 'stem'\n",
    "        try:\n",
    "            data = pd.read_pickle(\"df_fe_\"+temp+\"_preprocessed_train.pkl\")\n",
    "            print(\"File loaded!!\")\n",
    "        except:\n",
    "            data[\"question1\"] = data[\"question1\"].fillna(\"\")\n",
    "            data[\"question2\"] = data[\"question2\"].fillna(\"\")\n",
    "            data[\"question1\"] = jb.Parallel(n_jobs=8)(jb.delayed(self.preprocess)(row) for row in tqdm(data[\"question1\"]))\n",
    "            data[\"question2\"] = jb.Parallel(n_jobs=8)(jb.delayed(self.preprocess)(row) for row in tqdm(data[\"question2\"]))\n",
    "            data.to_pickle(\"df_fe_\"+temp+\"_preprocessed_train.pkl\")\n",
    "        finally:\n",
    "            print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = Preprocessor(True)\n",
    "process.call_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic_features = df[df_basic_features.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqriH33Js6Hv"
   },
   "source": [
    "- Creating Word Cloud of Duplicates and Non-Duplicates Question pairs\n",
    "- We can observe the most frequent occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicate = df[df['is_duplicate'] == 1]\n",
    "dfp_nonduplicate = df[df['is_duplicate'] == 0]\n",
    "\n",
    "\n",
    "p = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\n",
    "n = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n",
    "\n",
    "print (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\n",
    "print (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n",
    "\n",
    "p=' '.join(list(map(lambda x:''.join(x),p)))\n",
    "n=' '.join(list(map(lambda x:''.join(x),n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the Stop Words:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"said\")\n",
    "stopwords.add(\"br\")\n",
    "stopwords.add(\" \")\n",
    "stopwords.remove(\"not\")\n",
    "\n",
    "stopwords.remove(\"no\")\n",
    "#stopwords.remove(\"good\")\n",
    "#stopwords.remove(\"love\")\n",
    "# stopwords.remove(\"like\")\n",
    "#stopwords.remove(\"best\")\n",
    "#stopwords.remove(\"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Clouds generated from duplicate pair question's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=len(p), stopwords=stopwords)\n",
    "wc.generate(p)\n",
    "print (\"Word Cloud for Duplicate Question pairs\")\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Clouds generated from non duplicate pair question's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=len(n),stopwords=stopwords)\n",
    "# generate word cloud\n",
    "wc.generate(n)\n",
    "print (\"Word Cloud for non-Duplicate Question pairs:\")\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "-  Interesting aspect of the questions is that a disproportionate number seem to be from/about India. This may cause all of the NLP features to skew towards words relevant to India questions.\n",
    "- I'm doubtful that the resulting features will generalize well to questions from/about different regions. Or we I may have to come up with more features to generalize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(enumerate(df_basic_features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler =StandardScaler()\n",
    "df_basic_features = scaler.fit_transform(df_basic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S = np.corrcoef(df_basic_features,rowvar=False)\n",
    "sns.heatmap(S,annot=True,fmt='f',figure=plt.figure(figsize=(15,10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "-  q1len and q1_n_words are very highly correlated and q2len and q2_n_words. I will drop these for now and add new features which are transformations of these features to deal with these.\n",
    "- I'm doubtful that the resulting features will generalize well to questions from/about different regions. Or we I may have to come up with more features to generalize this.\n",
    "- word_taotal has high correlation with  other features q1len, q2len, q1_n_words, q2_n_words.\n",
    "-  freq_qid1 and freq_qid2 are very highly correlated with freq_q1+q2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"q1len\",\"q1_n_words\",\"q2len\",\"q2_n_words\",\"word_Common\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df=pd.read_pickle(\"df_preprocessed.pkl\")\n",
    "    print(\"File Loaded!\")\n",
    "except:\n",
    "    df.to_pickle(\"df_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
